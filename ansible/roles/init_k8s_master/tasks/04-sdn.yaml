# - name: Aplica calico
#   shell: "kubectl --kubeconfig=/home/{{user}}/.kube/config apply -f https://projectcalico.docs.tigera.io/manifests/canal.yaml >> pod_network_setup.txt"
#   args:
#     creates: pod_network_setup.txt

# - name: Instalamos el operador de Tigera
#   shell: "kubectl  --kubeconfig=/home/{{user}}/.kube/config create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml"


# Tuve MUCHISIMOS problemas con el SDN. Al final tuve que bajarme este https://docs.projectcalico.org/manifests/calico.yaml 
# Y guardarlo en custom-resources.yaml y modificarlo descomentando la variable CALICO_IPV4POOL_CIDR y poniendo mi CIDR (como indica en la guía oficial de Calico https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel)
# Si lo hacía de cualquier otra forma, el haproxy se quedaba siempre en una red distinta (10.85.0.0/16) y el resto en la mía (192.168.0.0/16) y no era capaz de salir al exterior
# Tampoco se solucionaba reiniciando la máquina, como pone en la guía de devopslabs
# Aun así, la primera vez que levanta la app no pone ni el sdn ni el haproxy en mi CIDR, lo pone todo en la 10.85.0.0, 
# pero al menos, como todo está en la misma red, soy capaz de salir al exterior.
# Si se ejecuta una segunda vez el deploy.sh, al hacer el reset del cluster, ahí ya sí lo pone todo en mi CIDR (192.168.0.0)
# La verdad ahora mismo no sé decir por qué ocurre esto, pero con tiempo me gustaría investigarlo.

- name: Creando directorio de sdn
  file:
    path: sdn
    state: directory

- name: Copiamos el template al directorio creado
  template:
    src: custom-resources.yaml
    dest: sdn/custom-resources.yaml
    owner: "{{user}}"
    group: "{{user}}"
    mode: 0644

- name: Instalamos Calico
  shell: "kubectl --kubeconfig=/home/{{user}}/.kube/config apply -f sdn/custom-resources.yaml"

- name: Esperamos a que Calico se levante
  shell: "sleep 60"
